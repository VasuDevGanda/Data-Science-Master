{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c006f0",
   "metadata": {},
   "source": [
    "## Assignment: Feature Engineering-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79857eee",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16bcd67",
   "metadata": {},
   "source": [
    "**Ans:** MinMax Scaler shrinks the data within the given range, usually of 0 to 1. It transforms data by scaling features to a given range. It scales the values to a specific value range without changing the shape of the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d567bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.        ]\n",
      " [0.27272727 0.625     ]\n",
      " [0.         1.        ]\n",
      " [1.         0.75      ]]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "# import module\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "# create data\n",
    "data = [[11, 2], [3, 7], [0, 10], [11, 8]]\n",
    " \n",
    "# scale features\n",
    "scaler = MinMaxScaler()\n",
    "model=scaler.fit(data)\n",
    "scaled_data=model.transform(data)\n",
    " \n",
    "# print scaled features\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b12d35",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b518c8",
   "metadata": {},
   "source": [
    "**Ans:** The Unit Vector technique in feature scaling, also known as vector normalization or L2 normalization, is a method used to scale numerical features in a dataset to have a unit norm (i.e., a length of 1) along each feature dimension. It is commonly applied in machine learning algorithms and data preprocessing to ensure that all features are on a comparable scale, which can be particularly useful for algorithms sensitive to the magnitude of features.\n",
    "\n",
    "Unit Vector scaling ensures that each feature vector has a unit L2 norm, while Min-Max scaling scales the features to a predefined range. The choice between these scaling methods depends on the specific requirements of the machine learning algorithm and the characteristics of the data being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17dc9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98386991 0.17888544]\n",
      " [0.3939193  0.91914503]\n",
      " [0.         1.        ]\n",
      " [0.80873608 0.5881717 ]]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "# import module\n",
    "from sklearn.preprocessing import normalize\n",
    " \n",
    "# create data\n",
    "data = [[11, 2], [3, 7], [0, 10], [11, 8]]\n",
    " \n",
    "# scale features\n",
    "scaled_data = normalize(data)\n",
    " \n",
    "# print scaled features\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d30b7a",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3edc0c",
   "metadata": {},
   "source": [
    "**Ans:** PCA (Principal Component Analysis) is a widely used technique in the field of dimensionality reduction and data analysis. It is a mathematical procedure that transforms a high-dimensional dataset into a new coordinate system, in which the new axes (principal components) are orthogonal (uncorrelated) and ordered by the amount of variance they explain in the data.\n",
    "\n",
    "PCA is used in dimensionality reduction to simplify high-dimensional data by transforming it into a lower-dimensional space. It retains the most significant information while discarding less important aspects. This reduction aids in data visualization, computational efficiency, and noise reduction. By selecting the most relevant principal components, PCA creates a compressed representation of the original data, making it easier to analyze and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a89b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65147725],\n",
       "       [-0.27599663],\n",
       "       [-0.63001678],\n",
       "       [ 0.25453616]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37d93f",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232fe89",
   "metadata": {},
   "source": [
    "**Ans:** PCA is a specific technique used for feature extraction. Feature extraction aims to reduce the number of features in a dataset while retaining the most important information. PCA achieves this by transforming the original features into a new set of uncorrelated features called principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance they explain in the data. By selecting a subset of the top principal components, PCA effectively performs feature extraction.\n",
    "\n",
    "PCA is applied to a dataset with multiple features to derive a reduced set of principal components. These principal components serve as the new features that capture the most significant patterns and variation in the data. The number of principal components chosen determines the dimensionality of the reduced feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54b3281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[   30 50000    12]\n",
      " [   25 30000    10]\n",
      " [   35 70000    14]\n",
      " [   40 80000    16]\n",
      " [   28 40000    11]]\n",
      "\n",
      "Scaled Data:\n",
      "[[-0.30108397 -0.21566555 -0.27854301]\n",
      " [-1.24197138 -1.29399328 -1.2070197 ]\n",
      " [ 0.63980344  0.86266219  0.64993368]\n",
      " [ 1.58069085  1.40182605  1.57841037]\n",
      " [-0.67743894 -0.75482941 -0.74278135]]\n",
      "\n",
      "Principal Components:\n",
      "[[-0.45924307  0.06060353]\n",
      " [-2.16092973 -0.0584588 ]\n",
      " [ 1.24244359  0.17966586]\n",
      " [ 2.63344937 -0.1414239 ]\n",
      " [-1.25572015 -0.04038668]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example dataset with three features (age, income, education level)\n",
    "data = np.array([\n",
    "    [30, 50000, 12],\n",
    "    [25, 30000, 10],\n",
    "    [35, 70000, 14],\n",
    "    [40, 80000, 16],\n",
    "    [28, 40000, 11]\n",
    "])\n",
    "\n",
    "# Step 1: Standardize the data (important for PCA)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Step 2: Apply PCA for feature extraction\n",
    "num_components = 2  # Set the desired number of components\n",
    "pca = PCA(n_components=num_components)\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nScaled Data:\")\n",
    "print(data_scaled)\n",
    "\n",
    "print(\"\\nPrincipal Components:\")\n",
    "print(principal_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496abadf",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9216e4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[10.   4.5 30. ]\n",
      " [15.   4.2 25. ]\n",
      " [ 8.   4.8 35. ]\n",
      " [20.   4.  40. ]\n",
      " [12.   4.7 28. ]]\n",
      "\n",
      "Scaled Data:\n",
      "[[0.16666667 0.625      0.33333333]\n",
      " [0.58333333 0.25       0.        ]\n",
      " [0.         1.         0.66666667]\n",
      " [1.         0.         1.        ]\n",
      " [0.33333333 0.875      0.2       ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example dataset with features: price, rating, and delivery time\n",
    "data = np.array([\n",
    "    [10.0, 4.5, 30],\n",
    "    [15.0, 4.2, 25],\n",
    "    [8.0, 4.8, 35],\n",
    "    [20.0, 4.0, 40],\n",
    "    [12.0, 4.7, 28]\n",
    "])\n",
    "\n",
    "# Step 1: Create a Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Step 2: Fit and transform the data with Min-Max scaling\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nScaled Data:\")\n",
    "print(data_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d4259",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4457d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[1.0e+02 5.0e+01 1.0e+01 5.0e-01 5.0e+03]\n",
      " [1.5e+02 6.0e+01 1.2e+01 6.0e-01 6.0e+03]\n",
      " [8.0e+01 4.0e+01 8.0e+00 4.0e-01 4.0e+03]\n",
      " [1.2e+02 5.5e+01 1.1e+01 5.5e-01 5.5e+03]\n",
      " [9.0e+01 4.5e+01 9.0e+00 4.5e-01 4.5e+03]]\n",
      "\n",
      "Scaled Data:\n",
      "[[-0.32232919  0.          0.          0.          0.        ]\n",
      " [ 1.69222822  1.41421356  1.41421356  1.41421356  1.41421356]\n",
      " [-1.12815215 -1.41421356 -1.41421356 -1.41421356 -1.41421356]\n",
      " [ 0.48349378  0.70710678  0.70710678  0.70710678  0.70710678]\n",
      " [-0.72524067 -0.70710678 -0.70710678 -0.70710678 -0.70710678]]\n",
      "\n",
      "Principal Components:\n",
      "[[-1.41933436e-01  2.89397656e-01 -3.38685200e-16]\n",
      " [ 3.28460467e+00 -2.73876846e-01 -1.52180646e-16]\n",
      " [-3.03622116e+00 -2.32569053e-01  6.98831785e-17]\n",
      " [ 1.48262722e+00  1.88633940e-01  3.92577191e-16]\n",
      " [-1.58907730e+00  2.84143018e-02 -5.15504581e-17]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example dataset with multiple features (company financial data and market trends)\n",
    "data = np.array([\n",
    "    [100, 50, 10, 0.5, 5000],\n",
    "    [150, 60, 12, 0.6, 6000],\n",
    "    [80, 40, 8, 0.4, 4000],\n",
    "    [120, 55, 11, 0.55, 5500],\n",
    "    [90, 45, 9, 0.45, 4500]\n",
    "])\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Step 2: Apply PCA for dimensionality reduction\n",
    "num_components = 3  # Set the desired number of components\n",
    "pca = PCA(n_components=num_components)\n",
    "principal_components = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nScaled Data:\")\n",
    "print(data_scaled)\n",
    "\n",
    "print(\"\\nPrincipal Components:\")\n",
    "print(principal_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db494f1",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35305b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[ 1]\n",
      " [ 5]\n",
      " [10]\n",
      " [15]\n",
      " [20]]\n",
      "\n",
      "Scaled Data:\n",
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Given dataset\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8960637",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ced016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
